{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ff4f3c",
   "metadata": {},
   "source": [
    "### Task 2: Change Point Modeling and Insight Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a80168",
   "metadata": {},
   "source": [
    "#### Part 2.1: Core Analysis Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e5f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install gxx`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Bayesian change point analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [tau, means, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\gagur\\Videos\\Change_point_analysis_and_statistical_modeling_of_time_series_data\\env\\Lib\\site-packages\\rich\n",
       "\\live.py:256: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\gagur\\Videos\\Change_point_analysis_and_statistical_modeling_of_time_series_data\\env\\Lib\\site-packages\\rich\n",
       "\\live.py:256: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "def bayesian_change_point_analysis(df, max_changepoints=5):\n",
    "    \"\"\"\n",
    "    Perform Bayesian change point detection on oil price data\n",
    "    \"\"\"\n",
    "    prices = df['Price'].values\n",
    "    n_obs = len(prices)\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Priors for change points (uniform over time)\n",
    "        tau = pm.Uniform(\"tau\", lower=0, upper=n_obs, shape=max_changepoints)\n",
    "        \n",
    "        # Sort change points chronologically\n",
    "        sorted_tau = pm.Deterministic(\"sorted_tau\", pt.sort(tau))\n",
    "        \n",
    "        # Segment parameters\n",
    "        means = pm.Normal(\"means\", mu=0, sigma=10, shape=max_changepoints+1)\n",
    "        sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "        \n",
    "        # Determine regime for each observation\n",
    "        regime = np.zeros(n_obs, dtype=int)\n",
    "        for i in range(max_changepoints):\n",
    "            regime = regime + (np.arange(n_obs) >= sorted_tau[i])\n",
    "        \n",
    "        # Likelihood\n",
    "        pm.Normal(\n",
    "            \"likelihood\", \n",
    "            mu=means[regime], \n",
    "            sigma=sigma, \n",
    "            observed=prices\n",
    "        )\n",
    "        \n",
    "        # Sampling\n",
    "        trace = pm.sample(\n",
    "            draws=2000,\n",
    "            tune=1000,\n",
    "            chains=2,\n",
    "            target_accept=0.9,\n",
    "            return_inferencedata=True\n",
    "        )\n",
    "    \n",
    "    return trace\n",
    "\n",
    "def analyze_results(trace, df):\n",
    "    \"\"\"Analyze and visualize change point results\"\"\"\n",
    "    # Extract change points\n",
    "    tau_samples = trace.posterior[\"sorted_tau\"].values\n",
    "    tau_mean = np.mean(tau_samples, axis=(0, 1)).astype(int)\n",
    "    change_dates = df.iloc[tau_mean][\"Date\"]\n",
    "    \n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    \n",
    "    # Price series\n",
    "    ax.plot(df['Date'], df['Price'], label='Brent Oil Price')\n",
    "    \n",
    "    # Change points\n",
    "    for cp in change_dates:\n",
    "        ax.axvline(cp, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Detected Change Points in Brent Oil Prices')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Price (USD)')\n",
    "    plt.savefig('../outputs/figures/change_points.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return change_dates\n",
    "\n",
    "def quantify_impact(df, change_dates):\n",
    "    \"\"\"Quantify impact between change point regimes\"\"\"\n",
    "    results = []\n",
    "    prev_idx = 0\n",
    "    \n",
    "    for i, cp in enumerate(change_dates):\n",
    "        cp_idx = df[df['Date'] == cp].index[0]\n",
    "        segment = df.iloc[prev_idx:cp_idx]\n",
    "        \n",
    "        if len(segment) > 0:\n",
    "            results.append({\n",
    "                'start_date': df.iloc[prev_idx]['Date'],\n",
    "                'end_date': cp,\n",
    "                'mean_price': segment['Price'].mean(),\n",
    "                'volatility': segment['Price'].std(),\n",
    "                'duration_days': len(segment)\n",
    "            })\n",
    "        prev_idx = cp_idx\n",
    "    \n",
    "    # Add final segment\n",
    "    final_segment = df.iloc[prev_idx:]\n",
    "    results.append({\n",
    "        'start_date': df.iloc[prev_idx]['Date'],\n",
    "        'end_date': df.iloc[-1]['Date'],\n",
    "        'mean_price': final_segment['Price'].mean(),\n",
    "        'volatility': final_segment['Price'].std(),\n",
    "        'duration_days': len(final_segment)\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load processed data\n",
    "    df = pd.read_csv('../data/processed/cleaned_oil_prices.csv', parse_dates=['Date'])\n",
    "    \n",
    "    # Run change point detection\n",
    "    print(\"Running Bayesian change point analysis...\")\n",
    "    trace = bayesian_change_point_analysis(df)\n",
    "    \n",
    "    # Analyze results\n",
    "    change_dates = analyze_results(trace, df)\n",
    "    impact_df = quantify_impact(df, change_dates)\n",
    "    \n",
    "    # Save results\n",
    "    impact_df.to_csv('../outputs/change_point_impacts.csv', index=False)\n",
    "    print(\"Analysis complete. Results saved to outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593d389",
   "metadata": {},
   "source": [
    "### 2. Event Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_with_events(change_dates, events_file):\n",
    "    \"\"\"Correlate detected change points with known events\"\"\"\n",
    "    events = pd.read_csv(events_file, parse_dates=['Date'])\n",
    "    correlations = []\n",
    "    \n",
    "    for cp in change_dates:\n",
    "        # Find closest event within 30 days\n",
    "        time_diff = abs(events['Date'] - cp)\n",
    "        closest_event = events.loc[time_diff.idxmin()]\n",
    "        \n",
    "        if time_diff.min() <= pd.Timedelta(days=30):\n",
    "            correlations.append({\n",
    "                'change_point_date': cp,\n",
    "                'event_date': closest_event['Date'],\n",
    "                'event_name': closest_event['Event'],\n",
    "                'days_difference': time_diff.min().days\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(correlations)\n",
    "\n",
    "# Example usage after main analysis:\n",
    "# event_correlations = correlate_with_events(change_dates, '../data/processed/events_annotated.csv')\n",
    "# event_correlations.to_csv('../outputs/event_correlations.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
